---
title: "DWPR"
author: "Gerardo Martin"
date: "4 de abril de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

En este script veremos cómo hacer un modelo Poisson para sólo presencias, que es equivalente a un proceso Poisson de puntos. Comenzaremos leyendo las capas que generamos anteriormente y transformándolas a dataframe. Posteriormente transformaremos las presencias a formato ráster y de ráster a data.frame. 

A estas alturas es importantísimo aclarar que es necesario siempre trabajar en un sistema de coordenadas proyectado, pues necesitamos que cada píxel represente unidades de área iguales.

```{r}
library(raster); library(R2jags); library(parallel)

capas <- readRDS('capas-experimento.rds')
presencias <- readRDS('presencias.rds')

capas.df <- data.frame(rasterToPoints(capas))
presencias.r <- rasterize(presencias, capas[[1]], fun = 'count')
presencias.r[is.na(presencias.r[])] <- 0

presencias.df <- data.frame(rasterToPoints(presencias.r))

#Los pesos los especificamos con base en el tamaño del área de estudio. Otra manera es utilizar el tamaño de los píxeles (creo, jeje)
pesos <- rep(50 * 50, nrow(capas.df))

pesos[presencias.df$layer > 0] <- 1/pesos[1]

#Aquí es donde entramos los "sub-pesos" para las presencias
capas.df$pres <- presencias.df$layer/pesos

```

Pregunta del millón, cuál es el efecto de dividir el vector de presencias entre los pesos?

Ahora que ya tenermos la base de dato completa y lista para analizar continuamos con el modelo que pasaremos a JAGS. Hay dos alternativas para pasarle el modelo a JAGS, aquí sólo vamos a utilizar una que sirve para todas las implementaciones de JAGS, que son con un sólo procesador o en paralelo (tantos procesadores como tengamos, el procesamiento es mucho más rápido).

Por razones de tiempo, vamos a saltarnos el paso de elegir el modelo más óptimo, y para aprender algo más sobre la sintaxis de JAGS vamos a ver cómo especificar un modelo polinomial de 2o grado (términos cuadráticos)

El modelo lo especificamos como texto:
```{r}
modelString <- "model{

#Función de verosimilitud

for(i in 1:n){

      log(lambda[i]) <- alpha + beta.1 * layer.1 + beta.1.1 * pow(layer.1, 2) + 
                        beta.2 * layer.2 + beta.2.1 * pow(layer.2, 2) +
                        beta.3 * layer.3 + beta.3.1 * pow(layer.3, 2) +
                        beta.4 * layer.4 + beta.4.1 * pow(layer.4, 2) +
                        beta.5 * layer.5 + beta.5.1 * pow(layer.5, 2)

      lambda.w[i] <- lambda[i] * pesos[i]

      pres[i] ~ dpois(lambda.w[i])
}

#Distribuciones previas de los parámetros de regresión

      alpha ~ dnorm(0, 0.0001)
      beta.1 ~ dnorm(0, 0.0001)
      beta.1.1 ~ dnorm(0, 0.0001)
      beta.2 ~ dnorm(0, 0.0001)
      beta.2.1 ~ dnorm(0, 0.0001)
      beta.3 ~ dnorm(0, 0.0001)
      beta.3.1 ~ dnorm(0, 0.0001)
      beta.4 ~ dnorm(0, 0.0001)
      beta.4.1 ~ dnorm(0, 0.0001)
      beta.5 ~ dnorm(0, 0.0001)
      beta.5.1 ~ dnorm(0, 0.0001)

}"

writeLines(modelString, 'JAGS-model.txt')
```

Hay una manera más flexible de entrar el modelo, pero es menos ilustrativa de lo que el modelo está haciendo. La ventaja es que con la siguiente forma se pueden correr muchos modelos en lote. El truco está en especificar la matriz del modelo y pasar la matriz en la lista de datos para JAGS:

```{r}
modelString2 <- "model{

      for(i in 1:n){
            log(lambda[i]) <- inprod(beta[], X[i,])
            lambda.w[i] <- lambda[i] * pesos[i]
            pres[i] ~ dpois(lambda.w[i])
      }

      for(i in 1:nX){
            beta[i] ~ dnorm(0, 0.0001)
      }

}"

writeLines(modelString2, "JAGS-model-2.txt")
```

Ahora sólo vamos a trabajar con la segunda versión del modelo. Lo que sigue es formatear los datos. Para ello necesitamos crear una lista que contenga TODA la información contenida en el modelo especificado arriba:

```{r}

X.mat <- model.matrix( formula("~ layer.1 + I(layer.1^2) +
                        layer.2 + I(layer.2^2) +
                        layer.3 + I(layer.3^2) +
                        layer.4 + I(layer.4^2) +
                        layer.5 + I(layer.5^2)") ,capas.df)

lista.datos <- list(X = X.mat, pres = capas.df$pres, #Las presencias sub-pesadas
                  n = nrow(X.mat),  pesos = pesos,
                  nX = ncol(X.mat))

parametros <- c("beta")

```

Ahora ya estamos listos para correr JAGS. Simplemente hay un par de cosas más que considerar:

1. El número de iteraciones (n.iter)
2. La proporción de iteraciones que se guardarán (n.thin)
3. El número de iteraciones que se descartarán al inicio de las cadenas (n.burnin)
4. El número de cadenas (n.chains)

En un experimento del mundo real esto depende completamente de la convergencia, es decir, si el muestreador encontró las distribuciones "verdaderas" de los parámetros. Más tarde veremos cómo se hace esto:

Corriendo JAGS:

```{r}
modelo <- jags.parallel(data = lista.datos,
               model.file = "JAGS-model-2.txt",
               parameters.to.save = parametros,
               n.chains = 3,
               n.iter = 5000,
               n.thin = 5,
               n.burnin = 45,
               n.cluster = 3)
```

Una vez corrido el modelo necesitamos revisar que efectivamente hayamos encontrado los valores óptimos de los parámetros. El primer paso es revisar el parámetro "Rhat" y el número de muestras efectivas, lo cual siempre queda guardado en el resumen del modelo. Como regla de pulgar necesitamos que "Rhat" siempre valga menos de 1.1 y que el número de muestras efectivas sea mayor de 1000:

```{r}
print(modelo)
```

Después vemos cómo se comportaron las cadenas:

```{r}
library(ggmcmc)

ggs_traceplot(ggs(as.mcmc(modelo)))
ggs_density(ggs(as.mcmc(modelo)))
ggs_ppmean(ggs(as.mcmc(modelo)))
```


